{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SPACEMIT ONNXRUNTIME 神经网络部署软件栈 WorkShop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"compare.png\" alt=\"compare\" style=\"width:1280px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step1 准备模型（host端）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"onnx.png\" alt=\"onnx\" style=\"width:1280px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pytorch官方预训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "coyote: 23.9%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/lib/python3.8/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from torchvision.io import read_image\n",
    "from torchvision.models import mobilenet_v2, MobileNet_V2_Weights\n",
    "\n",
    "img = read_image(\"/onnx/tools/redwolf.jpg\")\n",
    "\n",
    "# Step 1: Initialize model with the best available weights\n",
    "weights = MobileNet_V2_Weights.DEFAULT\n",
    "model = mobilenet_v2(weights=weights)\n",
    "model.eval()\n",
    "\n",
    "# Step 2: Initialize the inference transforms\n",
    "preprocess = weights.transforms()\n",
    "\n",
    "# Step 3: Apply inference preprocessing transforms\n",
    "batch = preprocess(img).unsqueeze(0)\n",
    "\n",
    "# Step 4: Use the model and print the predicted category\n",
    "prediction = model(batch).squeeze(0).softmax(0)\n",
    "class_id = prediction.argmax().item()\n",
    "score = prediction[class_id].item()\n",
    "category_name = weights.meta[\"categories\"][class_id]\n",
    "print(f\"{category_name}: {100 * score:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step1.1 转换pt格式到onnx格式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/lib/python3.8/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/opt/miniconda3/lib/python3.8/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=MobileNet_V2_Weights.IMAGENET1K_V1`. You can also use `weights=MobileNet_V2_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================ Diagnostic Run torch.onnx.export version 2.0.1 ================\n",
      "verbose: False, log level: Level.ERROR\n",
      "======================= 0 NONE 0 NOTE 0 WARNING 0 ERROR ========================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "\n",
    "# step1 设置网络输入的形状\n",
    "dummy_input = torch.randn(1, 3, 224, 224, device=\"cpu\")\n",
    "model = torchvision.models.mobilenet_v2(pretrained=True).cpu()\n",
    "# step2 设置网络输入输出的name\n",
    "input_names = [ \"input\" ]\n",
    "output_names = [ \"output\" ]\n",
    "# step3 导出onnx模型\n",
    "torch.onnx.export(\n",
    "    model,\n",
    "    dummy_input,\n",
    "    \"/onnx/play_yard/mobilenet_v2.onnx\",\n",
    "    verbose=False,\n",
    "    input_names=input_names,\n",
    "    output_names=output_names,\n",
    "    export_params=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Serving '/onnx/play_yard/mobilenet_v2.onnx' at http://localhost:8080\n"
     ]
    }
   ],
   "source": [
    "import IPython\n",
    "import threading\n",
    "import time\n",
    "import os\n",
    "#!pip install netron\n",
    "\n",
    "def display_netron(path):\n",
    "    os.system(f'netron {path}')\n",
    "\n",
    "thread = threading.Thread(target=display_netron, args=(\"/onnx/play_yard/mobilenet_v2.onnx\",))\n",
    "thread.start()\n",
    "time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"1000\"\n",
       "            height=\"1000\"\n",
       "            src=\"http://localhost:8080\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7ff7e09e0df0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(IPython.display.IFrame(f\"http://localhost:8080\", width=1000, height=1000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step1.2 量化ONNX模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Please consider pre-processing before quantization. See https://github.com/microsoft/onnxruntime-inference-examples/blob/main/quantization/image_classification/cpu/ReadMe.md \n",
      "WARNING:root:Please consider pre-processing before quantization. See https://github.com/microsoft/onnxruntime-inference-examples/blob/main/quantization/image_classification/cpu/ReadMe.md \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calibrated and quantized model saved.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"/onnx/onnxruntime-inference-examples/quantization/image_classification/cpu/\")\n",
    "\n",
    "from onnxruntime.quantization import QuantFormat, QuantType, quantize_static\n",
    "import resnet50_data_reader\n",
    "\n",
    "# step1 指定网络与calibration数据集、calibration数据预处理方式\n",
    "input_model_path = \"/onnx/play_yard/mobilenet_v2.onnx\"\n",
    "output_model_path = \"/onnx/play_yard/mobilenet_v2_QDQ.onnx\"\n",
    "calibration_dataset_path = \"/onnx/onnxruntime-inference-examples/quantization/image_classification/cpu/test_images\"\n",
    "dr = resnet50_data_reader.ResNet50DataReader(calibration_dataset_path, input_model_path)\n",
    "\n",
    "# step2 设置参数并量化\n",
    "quantize_static(\n",
    "    input_model_path,\n",
    "    output_model_path,\n",
    "    dr,\n",
    "    quant_format=QuantFormat.QDQ,\n",
    "    per_channel=True,\n",
    "    weight_type=QuantType.QInt8,\n",
    ")\n",
    "print(\"Calibrated and quantized model saved.\")\n",
    "\n",
    "# step3 检查量化精度损失(TODO:)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Serving '/onnx/play_yard/mobilenet_v2_QDQ.onnx' at http://localhost:8081\n"
     ]
    }
   ],
   "source": [
    "def display_netron(path):\n",
    "    os.system(f'netron {path}')\n",
    "\n",
    "thread = threading.Thread(target=display_netron, args=(\"/onnx/play_yard/mobilenet_v2_QDQ.onnx\",))\n",
    "thread.start()\n",
    "time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"1000\"\n",
       "            height=\"1000\"\n",
       "            src=\"http://localhost:8081\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7ff7e09e0c10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(IPython.display.IFrame(f\"http://localhost:8081\", width=1000, height=1000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step2 部署执行（device端）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"mobile.png\" alt=\"mobile\" style=\"width:640px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step2.1 CPP代码编写"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "  //*************************************************************************\n",
    "  // 设置log等级\n",
    "  Ort::Env env(ORT_LOGGING_LEVEL_FATAL, \"test\");\n",
    "  //*************************************************************************\n",
    "  // 创建ort session\n",
    "  Ort::SessionOptions session_options;\n",
    "  session_options.AppendExecutionProvider(\"XNNPACK\");\n",
    "  session_options.SetIntraOpNumThreads(1);\n",
    "  Ort::Session session(env, model_path, session_options);\n",
    "  //*************************************************************************\n",
    "  // 打印网络的输入信息 (node names, types, shape etc.)\n",
    "  Ort::AllocatorWithDefaultOptions allocator;\n",
    "  // print number of model input nodes\n",
    "  const size_t num_input_nodes = session.GetInputCount();\n",
    "  std::vector<Ort::AllocatedStringPtr> input_names_ptr;\n",
    "  std::vector<const char*> input_node_names;\n",
    "  input_names_ptr.reserve(num_input_nodes);\n",
    "  input_node_names.reserve(num_input_nodes);\n",
    "  std::vector<int64_t> input_node_dims;  // simplify... this model has only 1 input node {1, 3, 224, 224}.\n",
    "                                         // Otherwise need vector<vector<>>\n",
    "\n",
    "  std::cout << \"Number of inputs = \" << num_input_nodes << std::endl;\n",
    "\n",
    "  // iterate over all input nodes\n",
    "  for (size_t i = 0; i < num_input_nodes; i++) {\n",
    "    // print input node names\n",
    "    auto input_name = session.GetInputNameAllocated(i, allocator);\n",
    "    std::cout << \"Input \" << i << \" : name =\" << input_name.get() << std::endl;\n",
    "    input_node_names.push_back(input_name.get());\n",
    "    input_names_ptr.push_back(std::move(input_name));\n",
    "\n",
    "    // print input node types\n",
    "    auto type_info = session.GetInputTypeInfo(i);\n",
    "    auto tensor_info = type_info.GetTensorTypeAndShapeInfo();\n",
    "\n",
    "    ONNXTensorElementDataType type = tensor_info.GetElementType();\n",
    "    std::cout << \"Input \" << i << \" : type = \" << type << std::endl;\n",
    "\n",
    "    // print input shapes/dims\n",
    "    input_node_dims = tensor_info.GetShape();\n",
    "    std::cout << \"Input \" << i << \" : num_dims = \" << input_node_dims.size() << '\\n';\n",
    "    for (size_t j = 0; j < input_node_dims.size(); j++) {\n",
    "      std::cout << \"Input \" << i << \" : dim[\" << j << \"] =\" << input_node_dims[j] << '\\n';\n",
    "    }\n",
    "    std::cout << std::flush;\n",
    "    input_node_dims[0] = 1;\n",
    "  }\n",
    "\n",
    "  constexpr size_t input_tensor_size = 224 * 224 * 3;  // simplify ... using known dim values to calculate size\n",
    "                                                       // use OrtGetTensorShapeElementCount() to get official size!\n",
    "\n",
    "  std::vector<float> input_tensor_values(input_tensor_size);\n",
    "  //*************************************************************************\n",
    "  // 获取并打印网络的输出 (node names, types, shape etc.)\n",
    "  auto output_node_names_ptr = session.GetOutputNameAllocated(0, allocator);\n",
    "  auto output_node_names = output_node_names_ptr.get();\n",
    "  std::cout << \"output_node_names: \" << output_node_names <<std::endl;\n",
    "  //*************************************************************************\n",
    "  // 初始化网络输入的数据\n",
    "  cnpy::NpyArray arr = cnpy::npy_load(\"image.npy\");\n",
    "  float* loaded_data = arr.data<float>();\n",
    "  std::cout << arr.shape[0] << \" x \" << arr.shape[1] << \" x \" << arr.shape[2] << \"\\n\";\n",
    "  for (unsigned int i = 0; i < input_tensor_size; i++) input_tensor_values[i] = static_cast<float>(loaded_data[i]);\n",
    "  // create input tensor object from data values\n",
    "  auto memory_info = Ort::MemoryInfo::CreateCpu(OrtArenaAllocator, OrtMemTypeDefault);\n",
    "  auto input_tensor = Ort::Value::CreateTensor<float>(memory_info, input_tensor_values.data(), input_tensor_size,\n",
    "                                                            input_node_dims.data(), 4);\n",
    "  assert(input_tensor.IsTensor());\n",
    "  std::vector<Ort::Value> output_tensors;\n",
    "  //*************************************************************************\n",
    "  // 网络推理计算\n",
    "    struct timeval start_time, stop_time;\n",
    "    gettimeofday(&start_time, nullptr);\n",
    "    // score model & input tensor, get back output tensor\n",
    "    output_tensors =\n",
    "        session.Run(Ort::RunOptions{nullptr}, input_node_names.data(), &input_tensor, 1, &output_node_names, 1);\n",
    "    assert(output_tensors.size() == 1 && output_tensors.front().IsTensor());\n",
    "    gettimeofday(&stop_time, nullptr);\n",
    "    double inference_time_ms = (get_us(stop_time) - get_us(start_time)) / 1000;\n",
    "    std::cout << \"Inference time for frame \" << \": \"\n",
    "                  << inference_time_ms << \" ms\"\n",
    "                  << \" XNNPACKrun: \" << \" - \" << \"ms\" << std::endl;\n",
    "  //*************************************************************************\n",
    "  // 获取网络输出\n",
    "  float* floatarr = output_tensors.front().GetTensorMutableData<float>();\n",
    "  std::string last_label = \"None\";\n",
    "  int argmax = -1;\n",
    "  //*************************************************************************\n",
    "  // 获取最大置信度与对应的标签，打印分类结果\n",
    "  auto it = std::max_element(floatarr, floatarr+1001);\n",
    "  argmax = std::distance(floatarr, it);\n",
    "  float prob_threshold = 0.2;\n",
    "  if ((argmax < labels.size()) && (*it > prob_threshold)) {\n",
    "    std::cout << \"label: \" << labels[argmax] << \" with probability \" << *it\n",
    "              << std::endl;\n",
    "    last_label = labels[argmax];\n",
    "  }\n",
    "  std::cout << std::flush;\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step2.2 部署执行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cannot find section .interp\n",
      "Load lable done... \n",
      "open failed(-1)\n",
      "tcm init err(-1)\n",
      "tcm_malloc alloc failed(524288)\n",
      "tcm alloc failed!\n",
      "malloc successfully!(0x740f80)(524288)\n",
      "Number of inputs = 1\n",
      "Input 0 : name =input\n",
      "Input 0 : type = 1\n",
      "Input 0 : num_dims = 4\n",
      "Input 0 : dim[0] =1\n",
      "Input 0 : dim[1] =3\n",
      "Input 0 : dim[2] =224\n",
      "Input 0 : dim[3] =224\n",
      "output_node_names: output\n",
      "224 x 224 x 3\n",
      "903168, 2816, 12544, 27, 32, 12544, 72, 32, 127872, 8, 1, 2816, 1, 1\n",
      "401408, 768, 12544, 32, 16, 12544, 32, 16, 130048, 4, 1, 768, 1, 1\n",
      "200704, 3072, 12544, 16, 96, 12544, 16, 96, 128000, 2, 1, 3072, 1, 1\n",
      "301056, 2688, 3136, 96, 24, 3136, 96, 24, 127488, 3, 1, 2688, 1, 1\n",
      "75264, 5760, 3136, 24, 144, 3136, 24, 144, 75264, 1, 1, 5760, 1, 1\n",
      "451584, 3840, 3136, 144, 24, 3136, 144, 24, 126720, 4, 1, 3840, 1, 1\n",
      "75264, 5760, 3136, 24, 144, 3136, 24, 144, 75264, 1, 1, 5760, 1, 1\n",
      "112896, 5120, 784, 144, 32, 784, 144, 32, 112896, 1, 1, 5120, 1, 1\n",
      "25088, 9216, 784, 32, 192, 784, 32, 192, 25088, 1, 1, 9216, 1, 1\n",
      "150528, 6656, 784, 192, 32, 784, 192, 32, 122880, 2, 1, 6656, 1, 1\n",
      "25088, 9216, 784, 32, 192, 784, 32, 192, 25088, 1, 1, 9216, 1, 1\n",
      "150528, 6656, 784, 192, 32, 784, 192, 32, 122880, 2, 1, 6656, 1, 1\n",
      "25088, 9216, 784, 32, 192, 784, 32, 192, 25088, 1, 1, 9216, 1, 1\n",
      "39936, 13312, 196, 192, 64, 208, 192, 64, 39936, 1, 1, 13312, 1, 1\n",
      "13312, 30720, 196, 64, 384, 208, 64, 384, 13312, 1, 1, 30720, 1, 1\n",
      "79872, 25600, 196, 384, 64, 208, 384, 64, 79872, 1, 1, 25600, 1, 1\n",
      "13312, 30720, 196, 64, 384, 208, 64, 384, 13312, 1, 1, 30720, 1, 1\n",
      "79872, 25600, 196, 384, 64, 208, 384, 64, 79872, 1, 1, 25600, 1, 1\n",
      "13312, 30720, 196, 64, 384, 208, 64, 384, 13312, 1, 1, 30720, 1, 1\n",
      "79872, 25600, 196, 384, 64, 208, 384, 64, 79872, 1, 1, 25600, 1, 1\n",
      "13312, 30720, 196, 64, 384, 208, 64, 384, 13312, 1, 1, 30720, 1, 1\n",
      "79872, 38400, 196, 384, 96, 208, 384, 96, 79872, 1, 1, 38400, 1, 1\n",
      "19968, 64512, 196, 96, 576, 208, 96, 576, 19968, 1, 1, 64512, 1, 1\n",
      "119808, 56832, 196, 576, 96, 208, 576, 96, 119808, 1, 1, 9472, 6, 1\n",
      "19968, 64512, 196, 96, 576, 208, 96, 576, 19968, 1, 1, 64512, 1, 1\n",
      "119808, 56832, 196, 576, 96, 208, 576, 96, 119808, 1, 1, 9472, 6, 1\n",
      "19968, 64512, 196, 96, 576, 208, 96, 576, 19968, 1, 1, 64512, 1, 1\n",
      "36864, 94720, 49, 576, 160, 64, 576, 160, 27648, 2, 1, 94720, 1, 1\n",
      "10240, 168960, 49, 160, 960, 64, 160, 960, 10240, 1, 1, 119680, 2, 1\n",
      "61440, 156160, 49, 960, 160, 64, 960, 160, 61440, 1, 1, 62464, 3, 1\n",
      "10240, 168960, 49, 160, 960, 64, 160, 960, 10240, 1, 1, 119680, 2, 1\n",
      "61440, 156160, 49, 960, 160, 64, 960, 160, 61440, 1, 1, 62464, 3, 1\n",
      "10240, 168960, 49, 160, 960, 64, 160, 960, 10240, 1, 1, 119680, 2, 1\n",
      "61440, 312320, 49, 960, 320, 64, 960, 320, 61440, 1, 1, 62464, 5, 1\n",
      "20480, 430080, 49, 320, 1280, 64, 320, 1280, 20480, 1, 1, 110208, 4, 1\n",
      "Inference time for frame : 682.722 ms XNNPACKrun:  - ms\n",
      "label: sundial with probability 13.7949\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# 重命名准备好的模型\n",
    "!cp mobilenet_v2_QDQ.onnx model.onnx\n",
    "\n",
    "# 拷贝预编译ONNXRUNTIME的动态库，并设置RPATH\n",
    "!cp /onnx/onnxruntime/build/Linux/RelWithDebInfo/libonnxruntime.so.1.15.1 .\n",
    "!patchelf --set-rpath /onnx/play_yard/ libonnxruntime.so.1.15.1\n",
    "!patchelf --set-interpreter /onnx/play_yard/ld-linux-riscv64-lp64d.so.1 libonnxruntime.so.1.15.1\n",
    "\n",
    "# 拷贝预编译的CPP Demo，并设置RPATH\n",
    "!cp /onnx/onnxruntime-inference-examples/c_cxx/build/squeezenet/capi_test .\n",
    "!patchelf --set-interpreter /onnx/play_yard/ld-linux-riscv64-lp64d.so.1 capi_test\n",
    "!patchelf --set-rpath /onnx/play_yard/ capi_test\n",
    "\n",
    "# qemu下仿真测试\n",
    "!qemu-riscv64 capi_test\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
